[
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "A Tutorial on Causal Inference with Error-Prone Variables",
    "section": "",
    "text": "In efforts to draw causal inferences from empirical data, researchers routinely address methodological challenges such as unobserved confounding, selection bias, and post-treatment variables. Yet, the problem of measurement error—particularly in key covariates or treatment variables—remains comparatively underexamined. Even when investigators include all theoretically relevant and observable confounders in their analytic models, the presence of measurement error compromises the validity of such adjustments. A covariate measured imprecisely no longer serves as a reliable control; rather, it behaves as if partially unobserved, thereby reintroducing bias into the causal estimation process. In this sense, the reliability of measurement serves not merely as a technical detail, but as a conceptual bridge between observed and unobserved confounding: the lower the reliability, the greater the extent to which measured covariates recede into the background as effectively unmeasured sources of bias.\nMeasurement error is an endemic feature of empirical work. Researchers often rely on imprecise instruments—be it due to respondent misreporting, interviewer error, or limitations of the survey instrument itself. For example, demographic variables may be inaccurately recorded through careless administration, proxy measures may be used for constructs that are difficult or impossible to observe directly, or the target variable is inherently latent, such as depressive symptoms or political ideology, and thus intrinsically noisy even under ideal measurement conditions.\nThis tutorial pursues two objectives. First, through a series of simple simulations, we demonstrate the consequences of measurement error in covariates and treatment variables on causal effect estimation. We show how such errors bias the target estimand and undermine standard analytic procedures. Second, we introduce two widely applicable adjustment strategies—regression calibration and control variates—that leverage validation subsamples containing accurate measurements. These techniques offer practical tools for mitigating the deleterious effects of measurement error and restoring inferential credibility.\n\n\nThroughout this tutorial, we adhere to a consistent notational convention. Let \\(D_i\\) denote the treatment assignment for unit \\(i\\), where \\(d \\in \\{0, 1\\}\\) in the binary case. While our discussion centers on binary treatments, the analytical framework extends in a straightforward manner to settings involving multi-valued or continuous treatments. The potential outcome corresponding to treatment level \\(d\\) for unit \\(i\\) is denoted by \\(Y_{di}\\). Under a binary treatment regime, the observed outcome follows the canonical switching equation: \\(Y_i = D_i Y_{1i} + (1 - D_i) Y_{0i}\\).\nWe represent observed covariates by \\(X_i\\) in the univariate case, and by \\(\\textbf{X}_i\\) when referring to a vector of pre-treatment characteristics for unit \\(i\\). To distinguish true values from their imperfect or proxy counterparts, we adopt the conventional notation of appending an asterisk. Accordingly, \\(X_i^*\\) denotes a noisy or error-prone measurement of \\(X_i\\), and \\(D_i^*\\) refers to a mismeasured or surrogate version of the treatment variable \\(D_i\\).\nFinally, in empirical contexts where validation subsamples are available—that is, subsets of the data for which gold-standard measurements of treatment or covariates are obtained—we define an indicator variable \\(S_i\\), such that \\(S_i = 1\\) if unit \\(i\\) is included in the validation sample, and \\(S_i = 0\\) otherwise."
  },
  {
    "objectID": "tutorial.html#notations",
    "href": "tutorial.html#notations",
    "title": "A Tutorial on Causal Inference with Error-Prone Variables",
    "section": "",
    "text": "Throughout this tutorial, we adhere to a consistent notational convention. Let \\(D_i\\) denote the treatment assignment for unit \\(i\\), where \\(d \\in \\{0, 1\\}\\) in the binary case. While our discussion centers on binary treatments, the analytical framework extends in a straightforward manner to settings involving multi-valued or continuous treatments. The potential outcome corresponding to treatment level \\(d\\) for unit \\(i\\) is denoted by \\(Y_{di}\\). Under a binary treatment regime, the observed outcome follows the canonical switching equation: \\(Y_i = D_i Y_{1i} + (1 - D_i) Y_{0i}\\).\nWe represent observed covariates by \\(X_i\\) in the univariate case, and by \\(\\textbf{X}_i\\) when referring to a vector of pre-treatment characteristics for unit \\(i\\). To distinguish true values from their imperfect or proxy counterparts, we adopt the conventional notation of appending an asterisk. Accordingly, \\(X_i^*\\) denotes a noisy or error-prone measurement of \\(X_i\\), and \\(D_i^*\\) refers to a mismeasured or surrogate version of the treatment variable \\(D_i\\).\nFinally, in empirical contexts where validation subsamples are available—that is, subsets of the data for which gold-standard measurements of treatment or covariates are obtained—we define an indicator variable \\(S_i\\), such that \\(S_i = 1\\) if unit \\(i\\) is included in the validation sample, and \\(S_i = 0\\) otherwise."
  },
  {
    "objectID": "tutorial.html#example-measurement-of-college-and-family-ses-with-proxy-variables",
    "href": "tutorial.html#example-measurement-of-college-and-family-ses-with-proxy-variables",
    "title": "A Tutorial on Causal Inference with Error-Prone Variables",
    "section": "Example: Measurement of College and Family SES with Proxy Variables",
    "text": "Example: Measurement of College and Family SES with Proxy Variables\nBefore proceeding to more formal discussions, it is helpful to consider a concrete example that illustrates the conceptual stakes of measurement error in causal inference. Consider a simplified model of the causal effect of college on future earnings as. In this context, one’s family socioeconomic status (SES) is a plausible confounding variable, as it influences both the likelihood of attending college and subsequent income. Represented as a DAG, this setup would include arrows from SES to both college attendance and earnings.\n\n\n\n(A) No Measurement Error\n\n\nNow consider a scenario in which the treatment variable—college education—is measured with error. Suppose the dataset includes a binary indicator, \\(CollegeDegree\\), denoting whether an individual obtained a degree. In a scenario where a student attended say 3 years of college but did not graduate, they would appear as untreated in the data but their exposure to college could still be influencing their future earnings. Thus, the treatment variable is measured with error. And this is identical to consdiering the following DAG as the causal system that should be considered to use to justify the assumptions.\n\n\n\n(B) Measurement Error in Treatment\n\n\nNext, consider measurement error in the confounder. SES is notoriously difficult to define precisely and even harder to measure comprehensively. For the case wherein we can’t measure the complete items related to the SES conprehensively we can rely on the data that should be similar but not necessaryily identical with the SES, such as FAFSA (Free Application for Federal Student Aid) records, which summarize family income and assets. While FAFSA data are informative, they capture only a subset of the multidimensional construct that SES entails. In such cases, we are adjusting for \\(FAFSA\\), a noisy or partial representation of the true confounder. The corresponding DAG would reflect this distinction, with an arrow from latent SES to its observable proxy.\n\n\n\n(C) Measurement Error in an Observed Confounder"
  },
  {
    "objectID": "tutorial.html#measurement-error-in-treatment-aka-attenuation-bias",
    "href": "tutorial.html#measurement-error-in-treatment-aka-attenuation-bias",
    "title": "A Tutorial on Causal Inference with Error-Prone Variables",
    "section": "Measurement Error in Treatment (aka Attenuation Bias)",
    "text": "Measurement Error in Treatment (aka Attenuation Bias)\nAlthough stylized and somewhat removed from many empirical contexts, consider a simplified case in which the causal structure is fully captured by the direct relationship \\(D \\to Y\\). In such a setting, where no confounding is present, merely observing the treatment variable \\(D\\) and the outcome \\(Y\\) is sufficient for causal identification. Researchers interested in estimating the causal effect of \\(D\\) on \\(Y\\) might then fit the following OLS model: \\[\nY_i = \\alpha + \\tau D_i + \\varepsilon_i,\n\\] where \\(\\tau\\) represents the causal estimand of interest. Now suppose, however, that the treatment variable \\(D_i\\) is measured with error, and the researcher observes only a noisy proxy \\(D_i^*\\). In this case, the model estimated in practice becomes: \\[\nY_i = \\alpha^{\\text{ep}} + \\tau^{\\text{ep}} D_i^* + \\varepsilon_i^{\\text{ep}},\n\\] where \\(\\tau^{\\text{ep}}\\) is the coefficient obtained from regressing \\(Y\\) on the error-prone version of the treatment. The key question is how this naive estimator \\(\\hat{\\tau}^{\\text{ep}}\\) compares to the true causal effect \\(\\tau\\) when the measurement error in \\(D_i^*\\) is ignored.\nTo illustrate this, we conduct a simulation where the true treatment effect is \\(\\tau = 2\\), and the continuous treatment variable \\(D\\) is measured with error such that \\(D^* = D + u\\), with \\(u \\sim \\mathcal{N}(0, 1)\\). Although \\(D\\) is treated as continuous for ease of interpretation, the underlying logic holds for binary treatments as well.\n\n\nCode\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse, patchwork)\n\nset.seed(2025)\n\nn &lt;- 1000\ntau_true &lt;- 2\nsigma_d &lt;- 1\nsigma_u &lt;- 1\nsigma_eps &lt;- 1\n\n# Data generation\nD &lt;- rnorm(n, 0, sigma_d)\nu &lt;- rnorm(n, 0, sigma_u)\nD_star &lt;- D + u\neps &lt;- rnorm(n, 0, sigma_eps)\nY &lt;- tau_true * D + eps\n\n# Create long-format data for ggplot\ndf &lt;- data.frame(\n  Y = Y,\n  D_true = D,\n  D_star = D_star\n)\n\n# Regression fits\nfit_true &lt;- lm(Y ~ D_true, data = df)\nfit_star &lt;- lm(Y ~ D_star, data = df)\n\n# Predictions for plotting regression lines\nd_seq &lt;- seq(-5, 5, length.out = 200)\npred_true &lt;- predict(fit_true, newdata = data.frame(D_true = d_seq))\npred_star &lt;- predict(fit_star, newdata = data.frame(D_star = d_seq))\n\ndf_lines &lt;- data.frame(\n  D = d_seq,\n  Y_true = pred_true,\n  Y_star = pred_star\n)\n\n\n\n\nCode\n# Plot\nggplot() +\n  geom_point(aes(x = D, y = Y), alpha = 0.2, color = \"black\") +\n  geom_point(aes(x = D_star, y = Y), alpha = 0.2, color = \"red\") +\n  geom_line(data = df_lines, aes(x = D, y = Y_true), color = \"black\", size = 2) +\n  geom_line(data = df_lines, aes(x = D, y = Y_star), color = \"red\", size = 2) +\n  labs(\n    x = \"D (or D*)\",\n    y = \"Y\"\n  ) +\n  scale_x_continuous(expand = c(0, 0), limits = c(-5, 5)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(-10, 10),\n                     breaks = c(-10, -7.5, -5, -2.5, 0, 2.5, 5, 7.5, 10)) +\n  theme_bw(base_size = 18) +\n  theme(panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\n\nIn the accompanying plot, the black dots and lines represent the case where the true value of \\(D\\) is observed, while the red points correspond to the error-prone measure \\(D^*\\). Due to the added noise, the red dots are more horizontally dispersed, although the vertical spread remains similar. This visualization highlights a key empirical consequence of classical measurement error in the treatment variable: it leads to a systematic underestimation of the true effect (true slope is \\(2\\), while the estimated slope with error-prone variable is \\(1\\)). This pattern is commonly known as attenuation bias, a well-documented result in econometrics literatures (Wooldridge 2012)."
  },
  {
    "objectID": "tutorial.html#measurement-error-in-an-observed-confounder",
    "href": "tutorial.html#measurement-error-in-an-observed-confounder",
    "title": "A Tutorial on Causal Inference with Error-Prone Variables",
    "section": "Measurement Error in an Observed Confounder",
    "text": "Measurement Error in an Observed Confounder\nWe now turn to the case in which measurement error arises not in the treatment variable, but in an observed confounder. In the ideal scenario—where all relevant variables are measured without error—the researcher would estimate the following OLS model: \\[\nY_i = \\alpha + \\tau D_i + \\gamma X_i + \\varepsilon_i,\n\\] where \\(D_i\\) is the treatment, \\(X_i\\) is a confounder, and \\(\\tau\\) represents the causal effect of interest.\nHowever, in the presence of measurement error, we observe only an error-prone proxy \\(X_i^*\\) rather than the true confounder \\(X_i\\). In that case, the estimated model becomes: \\[\nY_i = \\alpha^{\\text{ep}} + \\tau^{\\text{ep}} D_i + \\gamma^{\\text{ep}} X_i^* + \\varepsilon_i^{\\text{ep}}.\n\\]\nAs in the previous simulation, we model the measurement error in \\(X^*\\) using additive Gaussian noise: \\(X^* = X + u\\), where \\(u \\sim \\mathcal{N}(0, 1)\\). However, unlike the case of measurement error in the treatment—where the bias tends to attenuate the effect toward zero—measurement error in a confounder introduces more complex bias patterns. Specifically, the direction of the bias depends on the correlation between the treatment variable \\(D\\) and the true confounder \\(X\\).\nTo illustrate this, we set the true causal effect \\(\\tau\\) to 1 and examine two contrasting cases: one in which the treatment and the true confounder are positively correlated (\\(\\text{Corr}(D, X) = 0.7\\)), and another in which they are negatively correlated (\\(\\text{Corr}(D, X) = -0.7\\)).\n\n\nCode\nset.seed(2025)\nn &lt;- 1000\ntau &lt;- 1\ngamma &lt;- 2\n\nsimulate_case &lt;- function(cor_DX, label) {\n  D &lt;- rnorm(n)\n  X &lt;- cor_DX * D + sqrt(1 - cor_DX^2) * rnorm(n)\n  X_star &lt;- X + rnorm(n, sd = 1)\n  Y &lt;- tau * D + gamma * X + rnorm(n)\n  \n  # Residualize D w.r.t. X and X*\n  D_resid &lt;- residuals(lm(D ~ X))\n  D_resid_star &lt;- residuals(lm(D ~ X_star))\n  \n  data.frame(D, X, X_star, Y, label, D_resid, D_resid_star)\n}\n\n# Simulate and combine\ndf_all &lt;- bind_rows(\n  simulate_case(0.7, \"Corr(D,X) = +0.7\"),\n  simulate_case(-0.7, \"Corr(D,X) = -0.7\")\n)\n\n# Prediction line generator (uses D residualized on true X for plotting)\nget_lines_df &lt;- function(df, label) {\n  fit_true &lt;- lm(Y ~ D + X, data = df)\n  fit_star &lt;- lm(Y ~ D + X_star, data = df)\n  \n  d_seq &lt;- seq(min(df$D), max(df$D), length.out = 200)\n  x_fixed &lt;- mean(df$X)\n  x_star_fixed &lt;- mean(df$X_star)\n  \n  pred_true &lt;- predict(fit_true, newdata = data.frame(D = d_seq, X = x_fixed))\n  pred_star &lt;- predict(fit_star, newdata = data.frame(D = d_seq, X_star = x_star_fixed))\n  \n  d_resid_seq &lt;- d_seq - mean(df$D)\n  \n  data.frame(\n    D_resid = rep(d_resid_seq, 2),\n    Y = c(pred_true, pred_star),\n    Model = rep(c(\"True X\", \"Measured X*\"), each = length(d_seq)),\n    label = label\n  )\n}\n\n# Combine prediction lines\ndf_lines &lt;- bind_rows(\n  get_lines_df(df_all |&gt; filter(label == \"Corr(D,X) = +0.7\"), \"Corr(D,X) = +0.7\"),\n  get_lines_df(df_all |&gt; filter(label == \"Corr(D,X) = -0.7\"), \"Corr(D,X) = -0.7\")\n)\n\n# Plot: black = D residualized on X; red = D residualized on X*\np &lt;- ggplot() +\n  geom_point(data = df_all, aes(x = D_resid, y = Y), alpha = 0.1, color = \"black\") +\n  geom_point(data = df_all, aes(x = D_resid_star, y = Y), alpha = 0.1, color = \"red\") +\n  geom_line(data = df_lines, aes(x = D_resid, y = Y, color = Model, linetype = Model), size = 2) +\n  facet_wrap(~ label) +\n  scale_color_manual(values = c(\"True X\" = \"black\", \"Measured X*\" = \"red\")) +\n  scale_linetype_manual(values = c(\"True X\" = \"solid\", \"Measured X*\" = \"solid\")) +\n  labs(\n    x = \"Residualized D\", y = \"Y\", color = NULL, linetype = NULL\n  ) +\n  scale_x_continuous(expand = c(0, 0), limits = c(-3, 3)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(-10, 10)) +\n  coord_cartesian(xlim = c(-2.5, 2.5 )) +\n  theme_bw(base_size = 18) +\n  theme(panel.grid.minor = element_blank(),\n        legend.position = \"none\")\n\nprint(p)\n\n\n\n\n\n\n\n\n\nThe results reveal striking asymmetries. When \\(D\\) and \\(X\\) are negatively correlated, the estimated treatment effect is biased downward, underestimating the true effect. In contrast, when the correlation is positive, the bias shifts upward, leading to an overestimation of the effect. Notably, the measurement error process is held constant across both cases—simple additive Gaussian noise—yet its consequences for the treatment effect estimation are diametrically opposed."
  },
  {
    "objectID": "tutorial.html#parametric-approach-regression-calibration",
    "href": "tutorial.html#parametric-approach-regression-calibration",
    "title": "A Tutorial on Causal Inference with Error-Prone Variables",
    "section": "Parametric Approach: Regression Calibration",
    "text": "Parametric Approach: Regression Calibration\nRegression calibration represents one of the earliest algorithmic strategies for addressing measurement error, and it remains widely utilized due to its intuitive appeal and straightforward implementation. While originally not developed within the causal inference paradigm, once causal identification is established, regression calibration may be employed for the purpose of estimating point estimates and confidence intervals, as it serves fundamentally as a method of statistical adjustment.\nConsider the case in which the treatment variable is measured with error:\n\n\n\n\n\nLet us assume that for all units \\(i \\in \\{1, 2, \\ldots, N\\}\\), we observe the tuple \\((Y_i, D_i^*, X_i, S_i)\\), where \\(Y_i\\) denotes the observed outcome, \\(D_i^*\\) the error-prone measure of the treatment, \\(X_i\\) a pre-treatment confounder, and \\(S_i\\) an indicator variable signifying whether unit \\(i\\) belongs to the validation subsample. For a subset of the sample, indexed by \\(j \\in \\{1, 2, \\ldots, n\\}\\), we observe not only \\((Y_j, D_j^*, X_j)\\) but also the true treatment assignment \\(D_j\\), with \\(S_j = 1\\) indicating inclusion in the validation data.\nWe suppose the true data-generating process follows the linear model: \\[\nY_i = \\alpha + \\tau D_i + \\gamma X_i + \\varepsilon_i.\n\\]\nRegression calibration proceeds by first estimating the relationship between the true and error-prone variables using the validation data. Specifically, we posit a calibration model of the form: \\[\nD_j = h(D_j^*, X_j; \\gamma) + \\eta_j,\n\\] where \\(h(D_j^*, X_j; \\gamma)\\) is a parametric prediction function—typically a linear regression—used to approximate \\(\\mathbb{E}[D_j \\mid D_j^*, X_j]\\), and \\(\\eta_j\\) denotes a mean-zero stochastic error term.\nImportantly, the function \\(h(\\cdot)\\) is not assumed to recover the structural or causal relation between the mismeasured and true treatment but serves as a statistical approximation that captures the conditional expectation of the true variable given observed quantities. This model does not require strong structural assumptions—only that the validation data allow consistent estimation of the conditional mean.\nAfter estimating \\(\\widehat{\\gamma}\\) from the calibration model, we impute predicted values of the true treatment for the entire sample as: \\[\n\\widehat{D}_i = h(D_i^*, X_i; \\widehat{\\gamma}).\n\\]\nThese imputed values are then substituted in place of the unobserved \\(D_i\\) in the main regression model: \\[\nY_i = \\alpha + \\tau \\widehat{D}_i + \\gamma X_i + \\varepsilon_i.\n\\]\nConceptually, this approach resembles imputation methods commonly used in missing data analysis, where the validation sample provides the necessary information for recovering latent or mismeasured variables (Carroll et al. 2006).\nIt is worth noting that this regression calibration estimator is approximately consistent for \\(\\tau\\) under classical measurement error assumptions, especially when the measurement error is nondifferential and the calibration model is correctly specified. However, it may not be fully efficient, and standard errors must be corrected (e.g., via sandwich estimators or bootstrap) to account for the uncertainty in the imputed values.\nThe following simulation illustrates a case in which the treatment variable \\(D \\in \\{0,1\\}\\) is subject to classical misclassification, where the observed proxy \\(D^*\\) is generated by randomly flipping the true value with 30% probability. The true treatment \\(D\\) is not assigned at random but instead depends on a confounder \\(X \\sim \\mathcal{N}(0,1)\\), such that \\(\\mathbb{P}(D = 1 \\mid X)\\) follows a logistic function, thereby establishing \\(X\\) as a confounder influencing both treatment assignment and the outcome. The outcome follows the linear model \\(Y = \\alpha + \\tau D + \\gamma X + \\varepsilon\\), with \\(\\tau = 2\\) as the true causal effect of treatment. Because the observed treatment \\(D^*\\) is a noisy version of \\(D\\), regressing \\(Y\\) on \\(D^*\\) and \\(X\\) yields a biased (attenuated) estimate of \\(\\tau\\). To address this, regression calibration uses a validation subset in which \\(D\\) is observed to estimate a calibration model (via logistic regression) for \\(\\mathbb{P}(D = 1 \\mid D^*, X)\\), then imputes the conditional expectation \\(\\widehat{D} = \\mathbb{E}[D \\mid D^*, X]\\) for all units. Plugging \\(\\widehat{D}\\) into the main regression recovers an approximately unbiased estimate of \\(\\tau\\). The simulation results show that this approach substantially corrects the attenuation bias. However, since the imputed \\(\\widehat{D}\\) is itself an estimate—based on a finite validation set—this correction introduces additional variance, leading to a slight loss of efficiency compared to the naive estimator. This highlights the classic bias-variance tradeoff inherent in regression calibration: it reduces bias at the cost of increased variability.\n\n\nCode\nset.seed(2025)\n\n# Parameters\nN &lt;- 1000\nn_val &lt;- 200\nB &lt;- 1000\ntau &lt;- 2\ngamma &lt;- 1\nalpha &lt;- 0\ndelta_0 &lt;- 0\ndelta_1 &lt;- 1.5  # Strength of confounding (X -&gt; D)\n\n# Simulate data\nX &lt;- rnorm(N)\nprob_D &lt;- plogis(delta_0 + delta_1 * X)  # logistic(X) for treatment probability\nD &lt;- rbinom(N, 1, prob_D)                # True treatment influenced by X\nY &lt;- alpha + tau * D + gamma * X + rnorm(N)\n\n# Introduce 30% random flipping (misclassification)\nD_star &lt;- ifelse(runif(N) &lt; 0.3, 1 - D, D)\n\n# Validation indicator\nS &lt;- rep(0, N)\nval_idx &lt;- sample(1:N, n_val)\nS[val_idx] &lt;- 1\n\n# Bootstrap storage\nest_naive &lt;- numeric(B)\nest_rc &lt;- numeric(B)\n\nfor (b in 1:B) {\n  idx &lt;- sample(1:N, N, replace = TRUE)\n  \n  X_b &lt;- X[idx]\n  D_b &lt;- D[idx]\n  D_star_b &lt;- D_star[idx]\n  Y_b &lt;- Y[idx]\n  S_b &lt;- S[idx]\n  \n  # Naive regression\n  fit_naive &lt;- lm(Y_b ~ D_star_b + X_b)\n  est_naive[b] &lt;- coef(fit_naive)[\"D_star_b\"]\n  \n  # Logistic regression calibration in validation sample\n  calib_fit &lt;- glm(D_b ~ D_star_b + X_b, subset = S_b == 1, family = binomial)\n  D_hat_b &lt;- predict(calib_fit, newdata = data.frame(D_star_b = D_star_b, X_b = X_b), type = \"response\")\n  \n  # RC regression with imputed treatment probability\n  fit_rc &lt;- lm(Y_b ~ D_hat_b + X_b)\n  est_rc[b] &lt;- coef(fit_rc)[\"D_hat_b\"]\n}\n\n# Combine and plot\ndf_plot &lt;- data.frame(\n  estimate = c(est_naive, est_rc),\n  method = rep(c(\"Naive (D*)\", \"Regression Calibration\"), each = B)\n)\n\n# Plot\np &lt;- ggplot(df_plot, aes(x = estimate, fill = method)) +\n  geom_histogram(colour = \"white\", bins = 40, alpha = 0.7, position = \"identity\") +\n  geom_vline(xintercept = tau, linetype = 3, size = 1.5, color = \"black\") +\n  facet_wrap(~ method, scales = \"free\") +\n  labs(x = \"Estimated Treatment Effect\", y = \"Count\") +\n  scale_x_continuous(limits = c(0, 3.5)) +\n  theme_bw(base_size = 18) +\n  theme(panel.grid.minor = element_blank(),\n        legend.position = \"none\")\n\nprint(p)"
  },
  {
    "objectID": "tutorial.html#control-variates-approach",
    "href": "tutorial.html#control-variates-approach",
    "title": "A Tutorial on Causal Inference with Error-Prone Variables",
    "section": "Control Variates Approach",
    "text": "Control Variates Approach\nWhile regression calibration offers a simple and intuitive correction for measurement error by modeling the relationship between the mismeasured and true treatment values, its performance hinges critically on correctly specifying the calibration model and treating the imputed values as fixed in subsequent outcome estimation. This can be problematic in settings where the relationship between \\(D_i^*\\) and \\(D_i\\) is complex or nonlinear, or when the outcome model is itself nonlinear, such as in logistic regression. Moreover, regression calibration typically involves substituting \\(\\widehat{D}_i\\) for \\(D_i\\) in a plug-in fashion, which may result in biased or inefficient estimates when the calibration error is substantial or when variance from the imputation step is not properly accounted for. The control variates approach, by contrast, offers a more general and robust strategy: it corrects bias and improves efficiency by combining an unbiased estimator based on the validation sample with auxiliary estimators derived from the larger, error-prone sample, without requiring strong modeling assumptions about the measurement process. Crucially, it does so in a way that directly targets the efficiency loss from validation subsampling, while maintaining the consistency of the final estimator.\nConsider a setting in which the treatment indicator \\(D_i\\) is measured with error, such that we observe only a noisy proxy \\(D_i^*\\). However, for a subset of the sample—indexed by \\(S_i = 1\\)—we observe the true treatment value \\(D_i\\). Given that \\(D_i^*\\) is a noisy variable prone to attenuation bias, what is the most straightforward strategy for estimating the ATE in this setting?\nA simple approach is to restrict estimation to the validation subset for which the true treatment values \\(D_i\\) are observed. This yields an estimate of the conditional average treatment effect among the validated units: \\[\n\\tau_{\\text{val}} = \\mathbb{E}[Y_{1i} - Y_{0i} \\mid S_i = 1].\n\\] If the validation sample is selected completely at random—formally, if \\(Y_{1i}, Y_{0i}, D_i, D_i^*, X_i \\indep S_i\\)—then this conditional estimand is equal to the population ATE: \\[\n\\tau_{\\text{val}} = \\mathbb{E}[Y_{1i} - Y_{0i} \\mid S_i = 1] = \\mathbb{E}[Y_{1i} - Y_{0i}] = \\tau.\n\\] In this case, \\(\\hat{\\tau}_{\\text{val}}\\) is an unbiased estimator for \\(\\tau\\), i.e., \\(\\mathbb{E}[\\hat{\\tau}_{\\text{val}}] = \\tau\\). However, relying solely on the validation subset may be inefficient, especially when the subset is small. Although unbiased, \\(\\hat{\\tau}_{\\text{val}}\\) may suffer from high variance, resulting in wide confidence intervals and limited inferential value.\nThe control variates approach addresses this issue by starting with \\(\\hat{\\tau}_{\\text{val}}\\) and then leveraging auxiliary information from the full sample—where only the error-prone \\(D_i^*\\) is available—to improve estimation precision. The core idea is to extract the informative component of \\(D_i^*\\) in a way that reduces variance without introducing bias, thereby improving the asymptotic efficiency of the estimator.\nTo formalize this, let \\(\\hat{\\tau}_{\\text{val}}\\) denote a consistent estimator of the ATE obtained from the validation dataset, where the true treatment \\(D_i\\) is observed. Suppose also that we compute an error-prone estimator \\(\\hat{\\tau}_{\\text{main}}\\) from the full dataset using the mismeasured treatment \\(D_i^*\\). Although \\(\\hat{\\tau}_{\\text{main}}\\) is generally biased, it may still be highly correlated with \\(\\hat{\\tau}_{\\text{val}}\\). The key idea, following Yang and Ding (2020), is to construct an improved estimator of the form: \\[\n\\hat{\\tau}_{\\text{cv}} = \\hat{\\tau}_{\\text{val}} - \\Gamma^\\top V^{-1} (\\hat{\\tau}_{\\text{val,ep}} - \\hat{\\tau}_{\\text{main}}),\n\\] where \\(\\hat{\\tau}_{\\text{val,ep}}\\) is the error-prone estimator computed from the validation sample using only \\(D_i^*\\), and \\(\\Gamma\\) and \\(V\\) are the sample covariance and variance of \\((\\hat{\\tau}_{\\text{val}}, \\hat{\\tau}_{\\text{val,ep}} - \\hat{\\tau}_{\\text{main}})\\), respectively. Under mild regularity conditions, this estimator is consistent for the true ATE \\(\\tau\\), and asymptotically more efficient than \\(\\hat{\\tau}_{\\text{val}}\\) alone. Crucially, this result does not rely on \\(\\hat{\\tau}_{\\text{main}}\\) or \\(\\hat{\\tau}_{\\text{val,ep}}\\) being consistent for \\(\\tau\\); it only requires that they converge to the same finite limit, which is typically satisfied when both estimators are derived from the same estimation procedure applied to \\(D_i^*\\) and \\(X_i\\). In this way, the proposed control variates estimator leverages the stability and efficiency of \\(\\hat{\\tau}_{\\text{main}}\\) to reduce the variance of the unbiased but potentially noisy \\(\\hat{\\tau}_{\\text{val}}\\). This approach generalizes beyond linear regression and can be applied to a wide class of asymptotically linear estimators, including inverse probability weighting and augmented estimators, as long as the difference \\((\\hat{\\tau}_{\\text{val,ep}} - \\hat{\\tau}_{\\text{main}})\\) is estimable and asymptotically negligible.\nWe simulate a setting where the binary treatment variable is measured with error, and a small fraction of the data includes validation information containing the true treatment status. Covariates \\(X_1, X_2 \\sim \\mathcal{N}(0,1)\\) determine the true treatment \\(D\\) via a logistic model, and the observed treatment \\(D^*\\) is generated by flipping \\(D\\) with probability \\(15\\%\\). Potential outcomes are nonlinear functions of the covariates, and the true ATE is set to \\(2\\). We compare three estimators: (i) a estimator that fits a linear regression using only the validated subset; (ii) an estimator that uses the full sample but relies on the misclassified treatment \\(D^*\\); and (iii) a estimator that corrects the validation-only estimate by leveraging the discrepancy between the error-prone estimates in the full and validation samples. The simulation shows that the error-prone estimator exhibits clear attenuation bias despite its low variance. In contrast, both the validation-only and control variates estimators are approximately unbiased. Importantly, the control variates estimator consistently achieves lower variance than the validation-only estimator across all validation fractions, thereby demonstrating that incorporating auxiliary information from the error-prone full sample—while correcting for its bias—can yield a more efficient yet still unbiased estimate.\n\n\nCode\nset.seed(2025)\n\nsimulate_once &lt;- function(n, val_frac, misclass_rate = 0.15) {\n  X1 &lt;- rnorm(n)\n  X2 &lt;- rnorm(n)\n  p &lt;- plogis(X1 + 0.5 * X2)\n  D &lt;- rbinom(n, 1, p)\n  D_star &lt;- ifelse(runif(n) &lt; misclass_rate, 1 - D, D)\n  \n  # Define potential outcomes\n  Y0 &lt;- exp(X1) / 4 + sin(2 * X2) + rnorm(n)\n  Y1 &lt;- 2 + exp(X1) / 4 + sin(2 * X2) + rnorm(n)  # add ATE = 2\n  \n  # Reveal observed outcome\n  Y &lt;- ifelse(D == 1, Y1, Y0)\n  \n  # Validation indicator\n  S &lt;- rep(0, n)\n  S[sample(1:n, size = n * val_frac)] &lt;- 1\n  \n  df &lt;- data.frame(Y, D, D_star, X1, X2, S)\n  \n  tau_val &lt;- coef(lm(Y ~ D + X1 + X2, data = df |&gt; filter(S == 1)))[\"D\"]\n  tau_ep_main &lt;- coef(lm(Y ~ D_star + X1 + X2, data = df))[\"D_star\"]\n  tau_ep_val &lt;- coef(lm(Y ~ D_star + X1 + X2, data = df |&gt; filter(S == 1)))[\"D_star\"]\n  \n  setNames(c(tau_val, tau_ep_main, tau_ep_val),\n           c(\"tau_val\", \"tau_ep_main\", \"tau_ep_val\"))\n}\n\n# Settings\nn &lt;- 1000\nval_fracs &lt;- seq(0.1, 0.5, by = 0.1)\nn_sim &lt;- 1000\ntrue_tau &lt;- 2\n\n# Run and collect full simulation results\nall_sims &lt;- lapply(val_fracs, function(vf) {\n  sims &lt;- replicate(n_sim, simulate_once(n, vf), simplify = \"matrix\")\n  \n  tau_val_vec &lt;- sims[\"tau_val\", ]\n  tau_ep_main_vec &lt;- sims[\"tau_ep_main\", ]\n  tau_ep_val_vec &lt;- sims[\"tau_ep_val\", ]\n  control_variate &lt;- tau_ep_val_vec - tau_ep_main_vec\n  b_hat &lt;- cov(tau_val_vec, control_variate) / var(control_variate)\n  tau_cv_vec &lt;- tau_val_vec - b_hat * control_variate\n  \n  data.frame(\n    val_frac = vf,\n    validation_only = tau_val_vec,\n    error_prone_main = tau_ep_main_vec,\n    control_variates = tau_cv_vec\n  )\n})\n\n# Reshape to long format\nall_sims_df &lt;- bind_rows(all_sims) |&gt;\n  pivot_longer(cols = c(\"validation_only\", \"error_prone_main\", \"control_variates\"),\n               names_to = \"estimator\", values_to = \"estimate\") |&gt;\n  mutate(bias = estimate - true_tau)\n\nbias_summary &lt;- all_sims_df |&gt;\n  group_by(val_frac, estimator) |&gt;\n  summarise(bias = mean(bias), variance = var(estimate), .groups = \"drop\") |&gt; \n  mutate(RMSE = sqrt(bias^2 + variance)) |&gt; \n  mutate(estimator = factor(estimator, levels = c(\"control_variates\", \"validation_only\", \"error_prone_main\"))) |&gt;\n  mutate(estimator = recode(estimator, \n                            validation_only = \"Validation Only\",\n                            error_prone_main = \"Error-Prone Main\",\n                            control_variates = \"Control Variates\"))\n\n\n# Create plots\np1 &lt;- bias_summary |&gt; \n  ggplot(aes(x = val_frac, y = bias, color = estimator, shape = estimator)) +\n  geom_line(size = .5) +\n  geom_point(size = 3) +\n  theme_bw(base_size = 18) +\n  theme(panel.grid.minor = element_blank()) +\n  labs(x = \"Validation sample fraction\", y = \"Bias\", color = NULL, shape = NULL)\n\np2 &lt;- ggplot(bias_summary, aes(x = val_frac, y = variance, color = estimator, shape = estimator)) +\n  geom_line(size = .5) +\n  geom_point(size = 3) +\n  theme_bw(base_size = 18) +\n  theme(panel.grid.minor = element_blank()) +\n  labs(x = \"Validation sample fraction\", y = \"Variance\", color = NULL, shape = NULL)\n\n# Combine with shared legend\np &lt;- (p1 | p2) + \n  plot_layout(guides = \"collect\") & \n  theme(legend.position = \"top\")\n\nprint(p)"
  }
]