---
title: "Causal Inference with Error-Prone Variables"
subtitle: "A Gentle Introduction and Bias Adjustment Using Validation Data"
author: 
  - Je Hoon Chae
  - Daniela R. Amaya
date: today
---

# Introduction

<<<<<<< Updated upstream
## Measurement Errors in DAGs and Potential Outcomes

### No Measurement Error
=======
## Notations

Throughout this tutorial, we adopt the following notation consistently. Let $D_i$ denote the treatment assignment status for unit $i$, where $d \in \{0, 1\}$ in the case of a binary treatment, though the framework can be extended to settings with multiple or continuous treatment levels. Let $Y_{di}$ represent the potential outcome for unit $i$ under treatment condition $d$. When the treatment is binary, the observed outcome for unit $i$ is given by the standard switching equation: $Y_i = D_i Y_{1i} + (1 - D_i) Y_{0i}$. We denote observed confounders using $X_i$ for a single variable, and $\textbf{X}_i$ for a row vector of multiple confounders for unit $i$. To indicate error-prone or proxy measures, we append an asterisk to the variable name. For example, $X_i^*$ denotes a noisy or proxy measure of $X_i$, and $D_i^*$ denotes a mismeasured/noisy/proxy version of the treatment variable $D_i$.

# Measurement Errors in DAGs and Potential Outcomes

## No Measurement Error
>>>>>>> Stashed changes

![](images/dag_no_error.svg){fig-align="center" width=40%}

- $X$ is an observed confounder
- Identification assumption: $Y_d \indep D \mid X$

---

## Measurement Error in Treatment Variable

![](images/dag_error_in_treatment.svg){fig-align="center" width=40%}

- We cannot observe $D$ directly; instead, we observe $D^*$
- $D^*$ is a proxy or noisy measurement of $D$
- Even if $Y_d \indep D \mid X$, identification via $D^*$ is not guaranteed: Does $Y_d \indep D^* \mid X$ hold?

---

## Measurement Error in Observed Confounder

![](images/dag_error_in_confounder.svg){fig-align="center" width=40%}

- We cannot observe $X$ directly; instead, we observe $X^*$
- $X^*$ is a proxy or noisy measurement of $X$
- We know that $Y_d \indep D \mid X$, but what about $Y_d \indep D  \mid X^*$?


# What if we ignore measurement error?

## When error-prone variable is a treatment variable

## When error-prone variable is an observed confounder

# Correcting Bias with Validation in a Subsample

While the problem of measurement error in the modern causal inference framework has received relatively recent attention, its correction has a long-standing tradition in fields such as statistics, econometrics, and psychometrics, particularly within the linear modeling framework. Broadly speaking, there are two main strategies for addressing this issue. The first involves collecting higher-quality measurements—often referred to as *validation data*—for a subset of the sample and using this information to correct bias in the full-sample analysis. The second strategy relies on prior knowledge or assumptions about the data-generating process. This typically entails imposing a parametric structure on the measurement error mechanism and using this structure to adjust the bias analytically or through simulation-based methods such as SIMEX [@cook1994simulation].

In this tutorial, we focus on two approaches that follow the first strategy, where a subset of the data contains accurate measurements of either the treatment variable $D$ or the confounder $X$, while the entire dataset includes only their error-prone proxies $D^*$ or $X^*$. We begin with regression calibration [@carroll2006measurement], a classical method rooted in linear regression that leverages validation data to correct for bias. We then introduce the control variates approach [@yang2020combining; @barnatchez2024flexible], which offers greater flexibility. Unlike regression calibration, this method does not require strong parametric assumptions about the measurement error process and can be integrated with modern estimators such as inverse probability weighting (IPW) or augmented inverse probability weighting (AIPW).

## Parametric approach: Regression calibration

## Control variates approach

Consider a setting in which the treatment indicator $D_i$ is measured with error, such that we observe only a noisy proxy $D_i^*$. However, for a subset of the sample—indexed by $S_i = 1$—we observe the true treatment value $D_i$. Given that $D_i^*$ is a noisy variable prone to attenuation bias, what is the most straightforward strategy for estimating the average treatment effect (ATE) in this setting?

A simple approach is to restrict estimation to the validation subset for which the true treatment values $D_i$ are observed. This yields an estimate of the conditional average treatment effect among the validated units:
$$
\tau_{\text{val}} = \mathbb{E}[Y_{1i} - Y_{0i} \mid S_i = 1].
$$
If the validation sample is selected completely at random—formally, if $Y_{1i}, Y_{0i}, D_i, D_i^*, X_i \indep S_i$—then this conditional estimand is equal to the population ATE:
$$
\tau_{\text{val}} = \mathbb{E}[Y_{1i} - Y_{0i} \mid S_i = 1] = \mathbb{E}[Y_{1i} - Y_{0i}] = \tau.
$$
In this case, $\hat{\tau}_{\text{val}}$ is an unbiased estimator for $\tau$, i.e., $\mathbb{E}[\hat{\tau}_{\text{val}}] = \tau$. However, relying solely on the validation subset may be inefficient, especially when the subset is small. Although unbiased, $\hat{\tau}_{\text{val}}$ may suffer from high variance, resulting in wide confidence intervals and limited inferential value.

The control variates approach addresses this issue by starting with $\hat{\tau}_{\text{val}}$ and then leveraging auxiliary information from the full sample—where only the error-prone $D_i^*$ is available—to improve estimation precision. The core idea is to extract the informative component of $D_i^*$ in a way that reduces variance without introducing bias, thereby improving the asymptotic efficiency of the estimator.

@yang2020combining
